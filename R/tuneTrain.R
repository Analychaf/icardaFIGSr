
#' @title Splitting the Data, Tuning and Training the Data, and Making Predictions
#' @description Automatic function for tuning and training data, it returns a list containing a model object, data frame and plot.
#' @param data object of class "data.frame" with target variable and predictor variables.
#' @param y character. Target variable.
#' @param p numeric. Proportion of data to be used for training. Default: 0.7
#' @param method character. Type of model to use for classification or regression.
#' @param Length integer. Number of values to output for each tuning parameter. If \code{search = "random"} is passed to \code{\link[caret]{trainControl}} through \code{...}, this becomes the maximum number of tuning parameter combinations that are generated by the random search. Default: 10.
#' @param control character. Resampling method to use. Choices include: "boot", "boot632", "optimism_boot", "boot_all", "cv", "repeatedcv", "LOOCV", "LGOCV", "none", "oob", timeslice, "adaptive_cv", "adaptive_boot", or "adaptive_LGOCV". Default: "repeatedcv". See \code{\link[caret]{train}} for specific details on the resampling methods.
#' @param number integer. Number of cross-validation folds or number of resampling iterations. Default: 10.
#' @param repeats integer. Number of folds for repeated k-fold cross-validation if "repeatedcv" is chosen as the resampling method in \code{control}. Default: 10.
#' @param summary expression. Computes performance metrics across resamples. For numeric \code{y}, the mean squared error and R-squared are calculated. For factor \code{y}, the overall accuracy and Kappa are calculated. See \code{\link[caret]{trainControl}} and \code{\link[caret]{defaultSummary}} for details on specification and summary options. Default: multiClassSummary.
#' @param process character. Defines the pre-processing transformation of predictor variables to be done. Options are: "BoxCox", "YeoJohnson", "expoTrans", "center", "scale", "range", "knnImpute", "bagImpute", "medianImpute", "pca", "ica", or "spatialSign". See \code{\link[caret]{preProcess}} for specific details on each pre-processing transformation. Default: c('center', 'scale').
#' @param positive character. The positive class for the target variable if \code{y} is factor. Usually, it is the first level of the factor.
#' @param parallelComputing logical. indicates whether to also use the parallel processing. Default: False
#' @param classtype integer.indicates the number of classes of the traits.
#' @param ... additional arguments to be passed to \code{createDataPartition}, \code{trainControl} and \code{train} functions in the package \code{caret}.
#' @param imbalanceMethod Method for handling imbalanced data ("up", "down"). Default: NULL.
#' @param imbalanceThreshold Threshold to determine if data is imbalanced (numeric between 0 and 0.4). Default: 0.2.

#' @return A list object with results from tuning and training the model selected in \code{method}, together with predictions and class probabilities. The training and test data sets obtained from splitting the data are also returned.
#'
#' If \code{y} is factor, class probabilities are calculated for each class. If \code{y} is numeric, predicted values are calculated.
#'
#' A ROC curve is created if \code{y} is factor. Otherwise, a plot of residuals versus predicted values is created if \code{y} is numeric.
#'
#' \code{tuneTrain} relies on packages \code{caret}, \code{ggplot2} and \code{plotROC} to perform the modelling and plotting.
#' @details Types of classification and regression models available for use with \code{tuneTrain} can be found using \code{names(getModelInfo())}. The results given depend on the type of model used.
#'
#' In addition to Model object, Model quality, Tuning and training datasets,For classification models, class probabilities and ROC curve are given in the results. For regression models, Variable importance, predictions and residuals versus predicted plot are given. \code{y} should be converted to either factor if performing classification or numeric if performing regression before specifying it in \code{tuneTrain}.
#'
#' @author Zakaria Kehel, Bancy Ngatia, Khadija Aziz, Chafik Analy
#' @examples
#' \dontrun{
#' #' if(interactive()){
#' #' ## Reading local test datasets
#' 
#' data(DurumWheatDHEWC)
#' data(BarleyRNOWC)
#' data(septoriaDurumWC)
#' 
#' ## Binary classification of ST_S with balanced data
#' rf.ST_S <- tuneTrain(data = as.data.frame(septoriaDurumWC),
#'                      y =  'ST_S',
#'                      method = 'rf',
#'                      summary = multiClassSummary,
#'                      repeats = 3,
#'                      classProbs = TRUE)
#' 
#' 
#' ## Binary classification of RNO with imbalanced data
#' knn.RNO <- tuneTrain(data = BarleyRNOWC,
#'                      y =  'RNO',
#'                      method = 'knn',
#'                      summary = multiClassSummary,
#'                      imbalanceMethod ="up",
#'                      imbalanceThreshold = 0.2,
#'                      process = c("scale","center"),
#'                      classProbs = TRUE,
#'                      repeats = 3)
#'                       
#' ## Regression of DHE
#' svm.DHE <- tuneTrain(data = DurumWheatDHEWC,
#'                      y =  'DHE',
#'                      method = 'knn',
#'                      summary = defaultSummary,
#'                      classProbs = FALSE,
#'                      repeats = 3)
#' 
#' 
#' ## Multiclass classification of DHE Classes with imbalanced data
#' 
#' # Create DHE Classes from DurumWheatDHEWC dataset
#  DurumWheatDHEWC$DHE <- ifelse(DurumWheatDHEWC$DHE <=172, "1", 
#                               ifelse(DurumWheatDHEWC$DHE <= 180, "2", 
#                                      3))
#' # Run Classification
#' knn.DHE.Classes <- tuneTrain(data = DurumWheatDHEWC,
#'                              y =  'DHE',
#'                              method = 'knn',
#'                              summary = multiClassSummary,
#'                              imbalanceMethod ="up",
#'                              imbalanceThreshold = 0.2,
#'                              process = c("scale","center"),
#'                              classProbs = TRUE,
#'                              repeats = 3)
#' 
#'  }
#' @seealso
#'  \code{\link[caret]{createDataPartition}},
#'  \code{\link[caret]{trainControl}},
#'  \code{\link[caret]{train}},
#'  \code{\link[caret]{predict.train}},
#'  \code{\link[ROSE]{ovun.sample}},
#'  \code{\link[ggplot2]{ggplot}},
#'  \code{\link[pRoc]{auc}},
#'  \code{\link[pRoc]{roc}},
#'  \code{\link[pRoc]{ggroc}},
#' @rdname tuneTrain
#' @export
#' @importFrom caret createDataPartition trainControl train predict.train confusionmatrix Preprocess
#' @importFrom utils stack
#' @importFrom ggplot2 ggplot aes geom_histogram theme_bw scale_colour_brewer scale_fill_brewer labs coord_equal annotate geom_point
#' @importFrom stats resid
#' @importFrom ROSE ovun.sample
#' @umportFrom pRoc auc roc ggroc
#' @details Imbalance handling methods "up" and "down" use ,respectively, upsample() and downsample() from caret.

tuneTrain <- function(data, y, p = 0.7,
                      method = "knn",
                      imbalanceMethod = NULL, # Imbalanced data handling
                      imbalanceThreshold = 0.2, # Threshold for imbalance handling
                      parallelComputing = FALSE,
                      control = "repeatedcv",
                      Length =10,
                      number = 10, repeats = 10,
                      process = c('center', 'scale', "knnImpute"),
                      summary = multiClassSummary,
                      classProbs = FALSE, ...) {
  
  # Validate imbalanceThreshold
  if (!classProbs && is.null(imbalanceMethod) && (imbalanceThreshold < 0 || imbalanceThreshold > 0.4)) {
    stop("Error: ImbalanceThreshold must be between 0 and 0.4. No Data imbalance handling is applied.")
  } 
  
  # Ensure target variable exists in the dataset
  if (!y %in% names(data)) {
    stop("Error: Target variable not found in the dataset.")
  }
  
  # Set a seed for reproducibility
  set.seed(1234)
  
  # Split predictors and target variable
  x <- data[which(colnames(data) != y)]
  yvec <- data[[y]]
  
  # Split data into training and test sets
  trainIndex <- caret::createDataPartition(y = yvec, p = p, list = FALSE)
  data.train <- data[trainIndex, ]
  data.test <- data[-trainIndex, ]
  
  # Preprocess: Impute missing values and apply scaling
  preProcValues <- caret::preProcess(data.train, method = process)
  data.train <- predict(preProcValues, data.train)
  data.test <- predict(preProcValues, data.test)
  
  # Class probability conversion if required
  if (classProbs && all(unique(data.train[[y]]) %in% 1:9)) {
    data.train[[y]] <- as.factor(paste("Cl", data.train[[y]], sep = "_"))
    data.test[[y]] <- as.factor(paste("Cl", data.test[[y]], sep = "_"))
  }
  
  # Filter necessary columns for trainx and testx after preprocessing
  trainx <- data.train[, colnames(data.train) %in% colnames(x), drop = FALSE]
  testx <- data.test[, colnames(data.test) %in% colnames(x), drop = FALSE]
  
  # Extract target variable for training and test sets
  trainy <- data.train[[y]]
  testy <- data.test[[y]]
  
  # Load required packages
  require(caret)
  require(doParallel)
  require(foreach)
  
  # Parallel computing initialization if enabled
  if (parallelComputing) {
    cores <- detectCores()
    cls <- makeCluster(max(1, cores - 1)) # Reserve at least 1 core
    registerDoParallel(cls)
    
    on.exit({
      if (exists("cls")) {
        stopCluster(cls)
      }
    }, add = TRUE)
  }
  
  # Determine subsampling method based on class probabilities and imbalance method
  subsampling <- if (classProbs && !is.null(imbalanceMethod)) {
    imbalanceMethod
  } else {
    NULL
  }
  
  ctrl = caret::trainControl(method = control,
                             number = number, 
                             repeats = repeats,
                             sampling = subsampling,
                             summaryFunction = summary, 
                             classProbs = classProbs)
  
  if (method == "treebag") {
    tune.mod = caret::train(trainx, trainy, method = method, 
                            tuneLength = Length, trControl = ctrl)
    train.mod <- tune.mod
    print(train.mod)
  }
  
  else if (method == "nnet") {
    tune.mod = caret::train(trainx, trainy, method = method, 
                            tuneLength = Length, trControl = ctrl, trace = FALSE)
    print(tune.mod)
    size <- tune.mod[["bestTune"]][["size"]]
    
    if (size - 1 <= 0) {
      seqStart <- size
    }
    else {
      seqStart <- size - 1
    }
    
    seqStop <- size + 1
    seqInt <- 1
    tuneGrid <- expand.grid(.size = seq(seqStart, seqStop, by=seqInt), 
                            .decay = 0.1^(seq(0.01, 0.09, 0.01))*0.11)
    
    ctrl2 = caret::trainControl(method = control, number = number, 
                                repeats = repeats, classProbs = classProbs,
                               sampling = subsampling,
                                summaryFunction = summary)
    
    train.mod = caret::train(trainx, trainy, method, 
                             tuneGrid = tuneGrid, tuneLength = Length, 
                             trControl = ctrl2, trace = FALSE)
    print(train.mod)
  }
  else {
    tune.mod = caret::train(trainx, trainy, method = method,
                            tuneLength = Length, trControl = ctrl)
    print(tune.mod)
    
    if (method == "knn") {
      k <- tune.mod[["bestTune"]][["k"]]
      
      if (k - 2 <= 0) {
        seqStart <- k
      }
      else {
        seqStart <- k - 2
      }
      seqStop <- k + 2
      seqInt <- 1
      tuneGrid <- expand.grid(.k = seq(seqStart, seqStop, by=seqInt))
    }
    else if (method == "rf") {
      mtry <- tune.mod[["bestTune"]][["mtry"]]
      
      if (mtry - 2 <= 0) {
        seqStart <- mtry
      }
      else {
        seqStart <- mtry - 2
      }
      
      seqStop <- mtry + 2
      seqInt <- 1
      tuneGrid <- expand.grid(.mtry = seq(seqStart, seqStop, by=seqInt))
    }
    else if (method == "svmLinear2") {
      cost <- tune.mod[["bestTune"]][["cost"]]
      
      if (cost - 0.25 <= 0 || cost - 0.5 <= 0 || cost - 0.75 <= 0 || cost - 1 <= 0) {
        seqStart <- cost
      }
      else {
        seqStart <- cost - 1
      }
      seqStop <- cost + 1
      seqInt <- 0.25
      tuneGrid <- expand.grid(.cost = seq(seqStart, seqStop, by=seqInt))
      
    } 
    
    ctrl2 = caret::trainControl(method = control, number = number, 
                                repeats = repeats, classProbs = classProbs, 
                               sampling = subsampling,
                                summaryFunction = summary)
    
    train.mod = caret::train(trainx, trainy, method, 
                             tuneGrid = tuneGrid, tuneLength = Length,
                             trControl = ctrl2)
    print(train.mod)
  }
  
  
  ## For binary classification
  if (is.factor(data[[y]]) && length(unique(data[[y]])) == 2) {

    # Predict probabilities
    prob.mod <- as.data.frame(caret::predict.train(train.mod, testx, type = "prob"))
    
    # Generate histogram for class probabilities
    prob.newdf <- utils::stack(prob.mod)
    colnames(prob.newdf) <- c("Probability", "Class")
    prob.hist <- ggplot(prob.newdf, aes(x = Probability, group=Class, colour = Class, fill = Class)) +
      geom_histogram(alpha = 0.4, position = "identity", binwidth = 0.1) + 
      theme_bw() + 
      scale_colour_brewer(palette = "Dark2") + 
      scale_fill_brewer(palette = "Dark2") + 
      facet_wrap(~Class) +
      labs(y = "Count")

    # Model Evaluation
    predictions <- predict(train.mod, newdata = testx)
    confMatrix <- caret::confusionMatrix(predictions, testy) 
    
    # Calculate ROC curve and AUC
    roc_curve <- roc(response = testy, predictor = prob.mod[,2])
    auc_value <- auc(roc_curve)
    
    # Plot ROC curve
    roc_plot <- ggroc(roc_curve)
    roc_plot <- roc_plot + ggtitle(sprintf("ROC Curve (AUC = %.2f)", auc_value))
    
    
    # Return results
    results = list(
      Tuning = tune.mod, 
      Model = train.mod, 
      `Model quality` = confMatrix,
      Variableimportance =NULL ,
      ROC_Plot = roc_plot,
      `Class Probabilities` = prob.mod, 
      `Class Probabilities Plot` = prob.hist,
      `Training Data` = data.train, 
      `Test Data` = data.test
    )
    return(results)
  }
  
  ## For multiclass classification
  else if (is.factor(data[[y]]) && length(unique(data[[y]])) > 2) {
    
    prob.mod <- predict(train.mod, testx, type = "prob")
    
    # Check for missing values in probabilities which can indicate issues during training
    if (any(is.na(prob.mod))) {
      warning("Missing values found in predicted probabilities.")
    }
    
    # Multiclass ROC analysis
    roc_results <- multiclass.roc(testy, prob.mod)
    
    # Generating histograms for class probabilities
    prob.newdf <- utils::stack(as.data.frame(prob.mod))
    colnames(prob.newdf) <- c("Probability", "Class")
    prob.hist <- ggplot(prob.newdf, aes(x = Probability, fill = Class, color = Class)) +
      geom_histogram(alpha = 0.4, position = "identity", binwidth = 0.05) + 
      theme_bw() + 
      scale_colour_brewer(palette = "Paired") + 
      scale_fill_brewer(palette = "Paired") + 
      facet_wrap(~ Class) +
      labs(y = "Count")
    
    # Confusion matrix
    predictions <- predict(train.mod, newdata = testx)
    confMatrix <- confusionMatrix(predictions, testy)
    
    # ROC plots and AUC values for each class
    roc_plots <- lapply(roc_results$rocs, function(roc_obj) {
      ggroc(roc_obj) + ggtitle(sprintf("ROC Curve (AUC = %.2f)", roc_obj$auc))+
        theme_minimal() + 
        labs(color = "Class", fill = "Class")
    })
    
    # Return a list of all evaluation results
    results = list(
      ROC_Results = roc_results,
      ROC_Plots = roc_plots,
      Probabilities_Plot = prob.hist,
      Model_Quality = confMatrix,
      Predictions = predictions
    )
    
    return(results)
  }
  
  ## For regression  
  else if(is.numeric(data[[y]])) {
    
    pred.mod = caret::predict.train(train.mod, newdata = testx)
    # Calculate residuals against the test set
    resids = testy - pred.mod
    
    # Prepare residuals for plotting
    respred.df = data.frame(Residuals = resids, Predicted = pred.mod)
    
    # Prepare predictions for plotting
    actual.vs.predicted.df = data.frame(Actual = testy, Predicted = pred.mod)
    
    ## Model Quality metrics
    Quality_metrics <- postResample(pred = actual.vs.predicted.df$Predicted,
                                    obs = actual.vs.predicted.df$Actual)
    
    # Residulas plots
    respred.plot = ggplot2::ggplot(respred.df, ggplot2::aes(x = Predicted, y = Residuals)) +
      ggplot2::geom_point() +  
      ggplot2::theme_bw() +
      ggplot2::labs(x = "Predicted", y = "Residuals")
    
    # Plotting Predicted vs Actual values
   actual.vs.predicted.plot = ggplot2::ggplot(actual.vs.predicted.df, ggplot2::aes(x = Actual, y = Predicted)) +
      ggplot2::geom_point() +
      ggplot2::geom_smooth(method = "loess")+
      ggplot2::theme_bw() +
      ggplot2::labs(x = "Actual", y = "Predicted", title = "Predicted vs Actual Values")
    
    # Get Variable importance
    VarImportance <- plot(varImp(train.mod))
    
    # Gather results
    results = list(Tuning = tune.mod, 
                   Model = train.mod, 
                   Predictions = pred.mod, 
                   `Residuals Vs. Predicted Plot` = respred.plot, 
                   `Predicted vs Actual Plot` = actual.vs.predicted.plot,
                   Quality_metrics = Quality_metrics,
                   VariableImportance = VarImportance,
                   `Training Data` = data.train, 
                   `Test Data` = data.test)
    return(results)
  }
}

