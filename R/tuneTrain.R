
#' @title Splitting the Data, Tuning and Training the Data, and Making Predictions
#' @description Automatic function for tuning and training data, it returns a list containing a model object, data frame and plot.
#' @param data object of class "data.frame" with target variable and predictor variables.
#' @param y character. Target variable.
#' @param p numeric. Proportion of data to be used for training. Default: 0.7
#' @param method character. Type of model to use for classification or regression.
#' @param length integer. Number of values to output for each tuning parameter. If \code{search = "random"} is passed to \code{\link[caret]{trainControl}} through \code{...}, this becomes the maximum number of tuning parameter combinations that are generated by the random search. Default: 10.
#' @param control character. Resampling method to use. Choices include: "boot", "boot632", "optimism_boot", "boot_all", "cv", "repeatedcv", "LOOCV", "LGOCV", "none", "oob", timeslice, "adaptive_cv", "adaptive_boot", or "adaptive_LGOCV". Default: "repeatedcv". See \code{\link[caret]{train}} for specific details on the resampling methods.
#' @param number integer. Number of cross-validation folds or number of resampling iterations. Default: 10.
#' @param repeats integer. Number of folds for repeated k-fold cross-validation if "repeatedcv" is chosen as the resampling method in \code{control}. Default: 10.
#' @param summary expression. Computes performance metrics across resamples. For numeric \code{y}, the mean squared error and R-squared are calculated. For factor \code{y}, the overall accuracy and Kappa are calculated. See \code{\link[caret]{trainControl}} and \code{\link[caret]{defaultSummary}} for details on specification and summary options. Default: multiClassSummary.
#' @param process character. Defines the pre-processing transformation of predictor variables to be done. Options are: "BoxCox", "YeoJohnson", "expoTrans", "center", "scale", "range", "knnImpute", "bagImpute", "medianImpute", "pca", "ica", or "spatialSign". See \code{\link[caret]{preProcess}} for specific details on each pre-processing transformation. Default: c('center', 'scale').
#' @param positive character. The positive class for the target variable if \code{y} is factor. Usually, it is the first level of the factor.
#' @param parallelComputing logical. indicates whether to also use the parallel processing. Default: False
#' @param classtype integer.indicates the number of classes of the traits.
#' @param ... additional arguments to be passed to \code{createDataPartition}, \code{trainControl} and \code{train} functions in the package \code{caret}.
#' @param imbalanceMethod Method for handling imbalanced data ("none", "over", "under", "nearmiss"). Default: "none".
#' @param imbalanceThreshold Threshold to determine if data is imbalanced (numeric between 0 and 1). Default: 0.2.

#' @return A list object with results from tuning and training the model selected in \code{method}, together with predictions and class probabilities. The training and test data sets obtained from splitting the data are also returned.
#'
#' If \code{y} is factor, class probabilities are calculated for each class. If \code{y} is numeric, predicted values are calculated.
#'
#' A ROC curve is created if \code{y} is factor. Otherwise, a plot of residuals versus predicted values is created if \code{y} is numeric.
#'
#' \code{tuneTrain} relies on packages \code{caret}, \code{ggplot2} and \code{plotROC} to perform the modelling and plotting.
#' @details Types of classification and regression models available for use with \code{tuneTrain} can be found using \code{names(getModelInfo())}. The results given depend on the type of model used.
#'
#' For classification models, class probabilities and ROC curve are given in the results. For regression models, predictions and residuals versus predicted plot are given. \code{y} should be converted to either factor if performing classification or numeric if performing regression before specifying it in \code{tuneTrain}.
#'
#' @author Zakaria Kehel, Bancy Ngatia, Khadija Aziz
#' @examples
#' \dontrun{
#' if(interactive()){
#'  data(septoriaDurumWC)
#'  knn.mod <- tuneTrain(data = septoriaDurumWC,y = 'ST_S',method = 'knn',positive = 'R')
#'  
#'  svm.mod <- tuneTrain(data = septoriaDurumWC, y = 'ST_S', method = 'svmLinear2', positive = 'R', 
#'                      imbalanceMethod = "Nearmiss", imbalanceThreshold = 0.1)
#'  
#'  nnet.mod <- tuneTrain(data = septoriaDurumWC,y = 'ST_S',method = 'nnet',positive = 'R')
#'
#'  }
#' }
#' @seealso
#'  \code{\link[caret]{createDataPartition}},
#'  \code{\link[caret]{trainControl}},
#'  \code{\link[caret]{train}},
#'  \code{\link[caret]{predict.train}},
#'  \code{\link[ggplot2]{ggplot}},
#'  \code{\link[plotROC]{geom_roc}},
#'  \code{\link[plotROC]{calc_auc}}
#' @rdname tuneTrain
#' @export
#' @importFrom caret createDataPartition trainControl train predict.train confusionmatrix 
#' @importFrom utils stack
#' @importFrom ggplot2 ggplot aes geom_histogram theme_bw scale_colour_brewer scale_fill_brewer labs coord_equal annotate geom_point
#' @importFrom plotROC geom_roc style_roc calc_auc
#' @importFrom stats resid
#' @importFrom ROSE ovun.sample
#' @details Imbalance handling methods "over", "under", and "nearmiss" use the ROSE package.


## For testing purpose, please make sure to install the ROSE package 
# install.packages("ROSE")


tuneTrain <- function (data, y, p = 0.7, method = method,
                       imbalanceMethod = "none", ## Imbalanced data handling ("over", "under", "nearmiss")
                       imbalanceThreshold = 0.2, ## Treshold to apply imbalanced data handlung
                       parallelComputing = F,length = 10, control = "repeatedcv",
                       number = 10,repeats = 10, process = c('center', 'scale'),
                       summary= multiClassSummary,positive, ...) 
{
  
  
  # Check if imbalanceThreshold is between 0 and 1
  if (imbalanceThreshold < 0 || imbalanceThreshold > 1) {
    stop("Error: imbalanceThreshold must be between 0 and 1.")
  }
  
  
  # Helper functions for imbalanced data handling
  
  ## Methode 1
  overSample <- function(data, target) {
    require(ROSE)
    data_balanced <- ovun.sample(as.formula(paste(target, "~ .")), data = data,
                                 method = "over")$data
    return(data_balanced)
  }
  
  ## Methode 2
  underSample <- function(data, target) {
    require(ROSE)
    data_balanced <- ovun.sample(as.formula(paste(target, "~ .")), data = data,
                                 method = "under")$data
    return(data_balanced)
  }
  
  
  ## Methode 3
  nearMissSample <- function(data, target) {
    require(ROSE)
    data_balanced <- ovun.sample(as.formula(paste(target, "~ .")), data = data,
                                 method = "under",
                                 N = NROW(data[target == unique(data[target])[1]]),
                                 p = 1, seed = 1, algorithm = "nearmiss1")$data
    return(data_balanced)
  }
  
  # Check for Imbalance in data
  if (!y %in% names(data)) {
    stop("Target variable not found in the dataset.")
  }
  classProportions <- table(data[[y]]) / nrow(data)
  
  # Ensure classProportions is not empty or NA before comparing
  if (!is.na(min(classProportions)) && length(classProportions) > 0 && 
      min(classProportions) / max(classProportions) < imbalanceThreshold) {
    if (imbalanceMethod == "over") {
      data <- overSample(data, y)
    } else if (imbalanceMethod == "under") {
      data <- underSample(data, y)
    } else if (imbalanceMethod == "nearmiss") {
      data <- nearMissSample(data, y)
    }
  }
  
  ## Data pre processing
    set.seed(1234) 
    x = data[which(colnames(data)!= y)]
    yvec = data[[y]]
    trainIndex = caret::createDataPartition(y = yvec, p = p,list = FALSE)
    
    data.train = as.data.frame(data[trainIndex, ])
    data.test = as.data.frame(data[-trainIndex, ])
    
    # Impute missing values using Knn
    preProcValues <- preProcess(data.train, method = c("center", "scale", "knnImpute"))
    data.train <- predict(preProcValues, data.train)
    data.test <- predict(preProcValues, data.test)
    
    ## Split train test sets
    split.data = list(trainset = data.train, testset = data.test)
    trainset = split.data$trainset
    testset = split.data$testset
    Train_Index <- row.names(data.train)
    trainx = trainset[colnames(trainset) %in% colnames(x)]
    trainy = trainset[[y]]
    testx = testset[colnames(testset) %in% colnames(x)]
    testy = testset[[y]]
    
    
    ## Model training
    require(caret)
    require(doParallel)
    
    if (parallelComputing == T) {
        cores <- detectCores()
        cls <- makeCluster(cores - 4)
        registerDoParallel(cls)
    }
    ctrl = caret::trainControl(method = control, number = number, 
                               repeats = repeats)
    
    if (method == "treebag") {
        tune.mod = caret::train(trainx, trainy, method = method, 
                                tuneLength = length, trControl = ctrl,
                                preProcess = process , ...)
        train.mod <- tune.mod
    }
    
    else if (method == "nnet") {
        tune.mod = caret::train(trainx, trainy, method = method, 
                                tuneLength = length, trControl = ctrl, 
                                preProcess = process, trace = FALSE)
        print(tune.mod)
        size <- tune.mod[["bestTune"]][["size"]]
        
        if (size - 1 <= 0) {
            seqStart <- size
        }
        else {
            seqStart <- size - 1
        }
        
        seqStop <- size + 1
        seqInt <- 1
        tuneGrid <- expand.grid(.size = seq(seqStart, seqStop, by=seqInt), 
                                .decay = 0.1^(seq(0.01, 0.08, 0.01))*0.11)
        
        ctrl2 = caret::trainControl(method = control, number = number, 
                                    repeats = repeats, classProbs = TRUE, 
                                    summaryFunction = summary)
        
        train.mod = caret::train(trainx, trainy, method, 
                                 tuneGrid = tuneGrid, tuneLength = length, 
                                 trControl = ctrl2, 
                                 preProcess = process, trace = FALSE, ...)
        print(train.mod)
    }
    else {
        tune.mod = caret::train(trainx, trainy, method = method,
                                tuneLength = length, trControl = ctrl,
                                preProcess = process)
        print(tune.mod)
        
        if (method == "knn") {
            k <- tune.mod[["bestTune"]][["k"]]
            
            if (k - 2 <= 0) {
                seqStart <- k
            }
            else {
                seqStart <- k - 2
            }
            seqStop <- k + 2
            seqInt <- 1
            tuneGrid <- expand.grid(.k = seq(seqStart, seqStop, by=seqInt))
        }
        else if (method == "rf") {
            mtry <- tune.mod[["bestTune"]][["mtry"]]
            
            if (mtry - 2 <= 0) {
                seqStart <- mtry
            }
            else {
                seqStart <- mtry - 2
            }
            
            seqStop <- mtry + 2
            seqInt <- 1
            tuneGrid <- expand.grid(.mtry = seq(seqStart, seqStop, by=seqInt))
        }
        else if (method == "svmLinear2") {
            cost <- tune.mod[["bestTune"]][["cost"]]
            
            if (cost - 0.25 <= 0 || cost - 0.5 <= 0 || cost - 0.75 <= 0 || cost - 1 <= 0) {
                seqStart <- cost
            }
            else {
                seqStart <- cost - 1
            }
            seqStop <- cost + 1
            seqInt <- 0.25
            tuneGrid <- expand.grid(.cost = seq(seqStart, seqStop, by=seqInt))
        }
        
        ctrl2 = caret::trainControl(method = control, number = number, 
                                    repeats = repeats, classProbs = TRUE, 
                                    summaryFunction = summary)
        
        train.mod = caret::train(trainx, trainy, method, 
                                 tuneGrid = tuneGrid, tuneLength = length,
                                 trControl = ctrl2, 
                                 preProcess = process, ...)
        print(train.mod)
    }
    if (parallelComputing == T) {
        stopCluster(cls)
        registerDoSEQ()
        
    } 
    
    
      ## For classification
    if (is.factor(data[[y]])) {
      if (missing(positive)) {
        warning("The positive class is not defined!", immediate. = TRUE, noBreaks. = T)
        positive <- readline(prompt="Please define the positive class for the target variable: ")
      }
      
      prob.mod <- as.data.frame(caret::predict.train(train.mod, testx, type = "prob"))
      prob.newdf <- utils::stack(prob.mod)
      colnames(prob.newdf) <- c("Probability", "Class")
      prob.hist <- ggplot2::ggplot(prob.newdf, ggplot2::aes(x = Probability,group=Class, colour = Class, fill = Class)) +
        ggplot2::geom_histogram(alpha = 0.4, position = "identity", binwidth = 0.1) + 
        ggplot2::theme_bw() + 
        ggplot2::scale_colour_brewer(palette = "Dark2") + 
        ggplot2::scale_fill_brewer(palette = "Dark2") + 
        ggplot2::facet_wrap( ~Class)+
        ggplot2::labs(y = "Count")
      
      testy_binary <- ifelse(testy == positive, 1, 0)
      
      # Extract predicted probabilities for the positive class
      prob.positive <- prob.mod[, positive]
      
      # Ensure length consistency
      if (length(testy_binary) != length(prob.positive)) {
        stop("Length of actual outcomes and predicted probabilities do not match.")
      }
      
     # Generate ROC curve after removing NA values
      roc_data <- data.frame(m = prob.positive, d = testy_binary)
      roc_data <- na.omit(roc_data)  # Remove rows with NA values
      
      if (nrow(roc_data) > 0) {
        g1 <- ggplot(roc_data, aes(m = m, d = d)) + plotROC::geom_roc() + 
          coord_equal() + plotROC::style_roc()
        plot.roc <- g1 + annotate("text", x = 0.75, y = 0.25, label = paste("AUC =", round(calc_auc(g1)$AUC, 4)))
        auc <- round(calc_auc(g1)$AUC, 4)
      } else {
        stop("Error in ROC data: No valid data after removing NA values.")
      }
      
      ## Model Evaluation
      
      # Generating predictions
      predictions <- predict(train.mod, newdata = testset)
      confMatrix <- confusionMatrix(data = predictions, reference = testset[[y]])
      
      # Extracting confusion matrix based metrics
      Model_Quality_metrics <- confMatrix
      
     # return results
        x = list(Tuning = tune.mod, 
                 Model = train.mod, 
                 `Model quality` = Model_Quality_metrics,
                 `Class Probabilities` = prob.mod, 
                 `Class Probabilities Plot` = prob.hist, 
                 `Area Under ROC Curve` = auc, 
                 `ROC Curve` = plot.roc,
                 `TrainingIndex`= Train_Index,
                 `Training Data` = trainset, 
                 `Test Data` = testset)
        return(x)
    }
    
    ## For regression  
    else if (is.numeric(data[[y]])) {
        if (missing(positive)) {
            warning("The positive class is not defined!", immediate. = TRUE, noBreaks. = T)
            positive <- readline(prompt="Please define the positive class for the target variable: ")
        }
      
        resids = resid(train.mod)
        pred.mod = caret::predict.train(train.mod, testx, 
                                        type = "raw")
        respred.df = data.frame(Residuals = resids, Predicted = pred.mod)
        respred.plot1 = ggplot2::ggplot(respred.df, ggplot2::aes(x = Predicted, 
                                                                 y = Residuals))
        respred.plot2 = respred.plot + ggplot2::geom_point(respred.df, 
                                       ggplot2::aes(x = Predicted, y = Residuals)) + 
            ggplot2::theme_bw() + ggplot2::labs(x = "Predicted", y = "Residuals")
        
        x = list(Tuning = tune.mod, 
                 Model = train.mod, 
                 Predictions = pred.mod, 
                 `Residuals Vs. Predicted Plot` = respred.plot2, 
                 `TrainingIndex`=Train_Index,
                 `Training Data` = trainset, 
                 `Test Data` = testset)
        return(x)
    }
}


## Testing example 

library(icardaFIGSr)
library(caret)

data(septoriaDurumWC)

svm.mod <- tuneTrain(data = septoriaDurumWC, y =  'ST_S',method = 'svmLinear2',positive = 'R', 
                     imbalanceMethod = "Nearmiss", imbalanceThreshold = 0.1)  




